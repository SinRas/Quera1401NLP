{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "quera_nlp.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xbw16lqxUxef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75c2f471-7ff6-4bd4-f0a7-6139073a0cb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 2.9 MB/s \n",
            "\u001b[?25hCollecting hazm\n",
            "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "\u001b[K     |████████████████████████████████| 316 kB 13.3 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.9.1-py2.py3-none-any.whl (211 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.21.5)\n",
            "Collecting libwapiti>=0.2.1\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 15.2 MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 16.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
            "Building wheels for collected packages: fasttext, nltk, libwapiti\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3131502 sha256=b3c5a64562a8cafcf8bc07d50bfc2a700334a9672f4c456631d2219d8bbfbc2c\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394489 sha256=f14bbc9742557be48bef950b3a8111ff20cac10845aa5e550b815c391edc4205\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=154525 sha256=f5a1b0e21757859b2a4205d7e196e652cc22c58fa70db3fdad2b0f0b9da731ce\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n",
            "Successfully built fasttext nltk libwapiti\n",
            "Installing collected packages: pybind11, nltk, libwapiti, hazm, fasttext\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed fasttext-0.9.2 hazm-0.7.0 libwapiti-0.2.1 nltk-3.3 pybind11-2.9.1\n"
          ]
        }
      ],
      "source": [
        "!pip install fasttext hazm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **import section**"
      ],
      "metadata": {
        "id": "e1EmlPStYtkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import codecs\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, balanced_accuracy_score, accuracy_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import fasttext\n",
        "from hazm import Normalizer, Lemmatizer, WordTokenizer, stopwords_list\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *"
      ],
      "metadata": {
        "id": "lqO0QRbgVlRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **metrics function**"
      ],
      "metadata": {
        "id": "y8QC2ZPQc-rd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_metrics(y_true, y_pred):\n",
        "  return {'accuracy': accuracy_score(y_true, y_pred), 'balance_accuracy': balanced_accuracy_score(y_true, y_pred)}"
      ],
      "metadata": {
        "id": "1LqHAvk2dPP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **data preparation**"
      ],
      "metadata": {
        "id": "EvjJqhDOfqsv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### reading data and split for train and test"
      ],
      "metadata": {
        "id": "5vzoJtRjY-_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://github.com/SinRas/Quera1401NLP/blob/main/sentipers.csv?raw=true')\n",
        "df_train, df_test = train_test_split(df, random_state=13)"
      ],
      "metadata": {
        "id": "7B4PZg-sVmPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### preprocessing and prepare labels"
      ],
      "metadata": {
        "id": "sFp39xvmZJRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = set(stopwords_list())\n",
        "normalizer = Normalizer()\n",
        "hazm_tokenizer = WordTokenizer()\n",
        "lemmatizer = Lemmatizer()\n",
        "def preprocess(text, remove_stopwords=False, lemmatize=False):\n",
        "    text = normalizer.normalize(text)\n",
        "    tokenized_words = hazm_tokenizer.tokenize(text)\n",
        "    if lemmatize:\n",
        "      tokenized_words = [lemmatizer.lemmatize(word).split('#')[0] for word in tokenized_words]\n",
        "    if remove_stopwords:\n",
        "      tokenized_words = [word for word in tokenized_words if word not in stopwords]\n",
        "    return ' '.join(tokenized_words)\n",
        "\n",
        "train_texts = [preprocess(t, True, True) for t in df_train['text']]\n",
        "test_texts = [preprocess(t, True, True) for t in df_test['text']]\n",
        "train_labels = list(df_train['label'].astype(int))\n",
        "test_labels = list(df_test['label'].astype(int))"
      ],
      "metadata": {
        "id": "XieunvRvYr7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **bag of words with classic ml**\n"
      ],
      "metadata": {
        "id": "WxXwsT6eatmm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[count vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
      ],
      "metadata": {
        "id": "inqZckUYgpV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(max_features=10000, ngram_range=(1,2)).fit(train_texts)\n",
        "x_train = vectorizer.transform(train_texts)\n",
        "x_test = vectorizer.transform(test_texts)"
      ],
      "metadata": {
        "id": "JCb8Nt6Yap7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### fit model\n",
        "\n",
        "[logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
        "\n",
        "[decision tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
        "\n",
        "[multi layer perceptron](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)"
      ],
      "metadata": {
        "id": "YYqYXPzxbAVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LogisticRegression(random_state=0, max_iter=500).fit(x_train, train_labels)"
      ],
      "metadata": {
        "id": "qwb8Mkwoc8FI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### test model"
      ],
      "metadata": {
        "id": "ZAwsgMxxerlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_preds = clf.predict(x_test)\n",
        "get_metrics(test_labels, lr_preds)"
      ],
      "metadata": {
        "id": "Db31Qyrxez7Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "192214bc-3ea7-45b6-d7f4-0d705a227377"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.6046926804386636, 'balance_accuracy': 0.46627937732783575}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **fasttext**"
      ],
      "metadata": {
        "id": "3mEiV8STjCVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **unsupervised**"
      ],
      "metadata": {
        "id": "HJdV99OcjbC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prepare fasttext data"
      ],
      "metadata": {
        "id": "ntr5ZzIRwOp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('fasttext_unsupervised', 'w', encoding='utf-8') as out_file:\n",
        "  for text in tramulti_classin_texts:\n",
        "    out_file.write(text+\"\\n\")"
      ],
      "metadata": {
        "id": "H0_K5LemjXu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train model\n",
        "\n",
        "[fasttext](https://fasttext.cc/docs/en/python-module.html)"
      ],
      "metadata": {
        "id": "66bmkAJXwVAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = fasttext.train_unsupervised('fasttext_unsupervised')\n"
      ],
      "metadata": {
        "id": "ZRW8f8IwwZon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get word vector"
      ],
      "metadata": {
        "id": "JqCryTDkww9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "v = model['کیفیت']\n",
        "v"
      ],
      "metadata": {
        "id": "eBH8ryiSw1xr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6825bba2-00b6-4c8d-f12d-8f57814564a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.08174179,  0.02283131, -0.20606835, -0.15568598, -0.01498603,\n",
              "       -0.21466225,  0.06228063, -0.02303107,  0.02668864,  0.4945953 ,\n",
              "       -0.28417784,  0.04637766,  0.05042268, -0.0167454 ,  0.02650889,\n",
              "        0.05333986,  0.02427731,  0.07475328,  0.10023021,  0.00977277,\n",
              "        0.2543495 ,  0.0171654 , -0.15237947, -0.0315598 ,  0.07570711,\n",
              "       -0.28095865,  0.2917196 ,  0.08144619, -0.07472356,  0.25846672,\n",
              "        0.05157208, -0.04778065, -0.04101653,  0.07141962, -0.16854843,\n",
              "       -0.23237541,  0.08648346, -0.15194973, -0.08310773,  0.1507775 ,\n",
              "        0.07506619,  0.3127003 ,  0.0302394 ,  0.05907486,  0.3800783 ,\n",
              "       -0.05382648, -0.24196891, -0.06017956, -0.06612045,  0.05207014,\n",
              "       -0.18682751, -0.0673539 , -0.1981201 ,  0.16740301, -0.02330521,\n",
              "       -0.00714078, -0.13274136,  0.0534226 , -0.0905301 ,  0.18917784,\n",
              "        0.01110097,  0.29115242,  0.33193773,  0.05842251,  0.09977193,\n",
              "       -0.24751475,  0.15286   , -0.0882332 ,  0.00382062, -0.20798872,\n",
              "       -0.2794076 , -0.01592098, -0.11834846, -0.24136278, -0.02269983,\n",
              "       -0.20924225,  0.5477654 , -0.08795848,  0.05569798, -0.048857  ,\n",
              "       -0.00494083, -0.1967047 ,  0.24793   , -0.20660417, -0.09532323,\n",
              "        0.1429201 ,  0.1357037 ,  0.24751626, -0.16854468,  0.14237255,\n",
              "        0.2321726 , -0.02513895,  0.35099855, -0.49682245, -0.07402401,\n",
              "       -0.40064347,  0.04135267,  0.09120075, -0.05688727,  0.132457  ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get similar words"
      ],
      "metadata": {
        "id": "6mL4iv0Hw_bc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_nearest_neighbors('کیفیت', k=10)"
      ],
      "metadata": {
        "id": "Pk0sMqAMxGOs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79a3f280-bc0f-4075-b27b-3e6092dae947"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.9502526521682739, 'با\\u200cکیفیت'),\n",
              " (0.9434821009635925, 'کیفیته'),\n",
              " (0.9333087801933289, 'باکیفیت'),\n",
              " (0.9270477294921875, 'فیلمبرداریش'),\n",
              " (0.9258806705474854, 'شفافیت'),\n",
              " (0.9210011959075928, 'عکسبرداری'),\n",
              " (0.9183793663978577, 'واضح'),\n",
              " (0.9171732664108276, 'روشن'),\n",
              " (0.9139121174812317, 'full'),\n",
              " (0.9123570919036865, 'عکس\\u200cبرداری')]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **supervised**"
      ],
      "metadata": {
        "id": "G9gzDQHjxV6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('fasttext_train', 'w', encoding='utf-8') as out_file:\n",
        "  for text, label in zip(train_texts, train_labels):\n",
        "    label = '__label__' + str(label)\n",
        "    out_file.write(label + ' ' + text + '\\n')\n",
        "\n",
        "with open('fasttext_test', 'w', encoding='utf-8') as out_file:\n",
        "  for text, label in zip(test_texts, test_labels):\n",
        "    label = '__label__' + str(label)\n",
        "    out_file.write(label + ' ' + text + '\\n')\n",
        "\n",
        "model = fasttext.train_supervised(input=\"fasttext_train\")\n",
        "# model.test('fasttext_test')"
      ],
      "metadata": {
        "id": "iI2nhJAXxNj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ft_preds = [int(i[0].split('_')[-1]) for i in model.predict(test_texts)[0]]\n",
        "get_metrics(test_labels, ft_preds)"
      ],
      "metadata": {
        "id": "AHvQQ0lE-Yoh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d5025a2-3e2f-469f-aa5e-2d02808f5487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.5608263198163733, 'balance_accuracy': 0.3845901284561627}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **deep lstm**"
      ],
      "metadata": {
        "id": "Dm4AxxtSx9zR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prepare data"
      ],
      "metadata": {
        "id": "KaRkYHFEAtis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 200\n",
        "max_words = 10000\n",
        "tokenizer = Tokenizer(split=' ', num_words=max_words)\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "x_train = tokenizer.texts_to_sequences(train_texts)\n",
        "x_train = pad_sequences(x_train, maxlen=max_len)\n",
        "x_test = tokenizer.texts_to_sequences(test_texts)\n",
        "x_test = pad_sequences(x_test, maxlen=max_len)\n",
        "y_train = LabelBinarizer().fit_transform(train_labels)\n",
        "y_test = LabelBinarizer().fit_transform(test_labels)\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train)"
      ],
      "metadata": {
        "id": "5AhY09P0-yKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### create model"
      ],
      "metadata": {
        "id": "6q-0pmhQAyDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 100\n",
        "lstm_out = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embed_dim, input_length=max_len))\n",
        "model.add(SpatialDropout1D(0.3))\n",
        "model.add(LSTM(lstm_out, dropout=0.3, recurrent_dropout=0.3, return_sequences=False))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "Gr14Oi4vAHZw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bff8681d-e731-43e6-a335-a9032a41a4fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 200, 100)          1000000   \n",
            "                                                                 \n",
            " spatial_dropout1d_3 (Spatia  (None, 200, 100)         0         \n",
            " lDropout1D)                                                     \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 100)               80400     \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 50)                5050      \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 5)                 255       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,085,705\n",
            "Trainable params: 1,085,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train model"
      ],
      "metadata": {
        "id": "HmDMVV9OA229"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, epochs = 50, batch_size=264, verbose = 2,\n",
        "          callbacks=[EarlyStopping(patience=5)], validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "id": "aev_81CPAfpC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87ac4d36-def6-458b-c17a-932df05d54e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "34/34 - 61s - loss: 1.4130 - accuracy: 0.3628 - val_loss: 1.3157 - val_accuracy: 0.3890 - 61s/epoch - 2s/step\n",
            "Epoch 2/50\n",
            "34/34 - 56s - loss: 1.3296 - accuracy: 0.3859 - val_loss: 1.3010 - val_accuracy: 0.4162 - 56s/epoch - 2s/step\n",
            "Epoch 3/50\n",
            "34/34 - 56s - loss: 1.2540 - accuracy: 0.4552 - val_loss: 1.1893 - val_accuracy: 0.4995 - 56s/epoch - 2s/step\n",
            "Epoch 4/50\n",
            "34/34 - 55s - loss: 1.0317 - accuracy: 0.5841 - val_loss: 1.1225 - val_accuracy: 0.5335 - 55s/epoch - 2s/step\n",
            "Epoch 5/50\n",
            "34/34 - 60s - loss: 0.8196 - accuracy: 0.6857 - val_loss: 1.1263 - val_accuracy: 0.5315 - 60s/epoch - 2s/step\n",
            "Epoch 6/50\n",
            "34/34 - 55s - loss: 0.6693 - accuracy: 0.7499 - val_loss: 1.1867 - val_accuracy: 0.5519 - 55s/epoch - 2s/step\n",
            "Epoch 7/50\n",
            "34/34 - 55s - loss: 0.5864 - accuracy: 0.7856 - val_loss: 1.2611 - val_accuracy: 0.5661 - 55s/epoch - 2s/step\n",
            "Epoch 8/50\n",
            "34/34 - 55s - loss: 0.5094 - accuracy: 0.8195 - val_loss: 1.3711 - val_accuracy: 0.5553 - 55s/epoch - 2s/step\n",
            "Epoch 9/50\n",
            "34/34 - 55s - loss: 0.4603 - accuracy: 0.8327 - val_loss: 1.3634 - val_accuracy: 0.5580 - 55s/epoch - 2s/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0eedf3ef90>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### test model"
      ],
      "metadata": {
        "id": "E6sTiZQiHVuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score,acc = model.evaluate(x_test, y_test, verbose = 2, batch_size = 64)"
      ],
      "metadata": {
        "id": "s13glEY6AphC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b383cad-dd0d-442a-8fb0-3cdd8ef1da27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "62/62 - 5s - loss: 1.3508 - accuracy: 0.5545 - 5s/epoch - 85ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pred_text(text):\n",
        "    tmp = [preprocess(text)]\n",
        "    seq = tokenizer.texts_to_sequences(tmp)\n",
        "    seq = pad_sequences(seq, maxlen=max_len)\n",
        "    y_pred = np.argmax(model.predict(seq), axis=1)\n",
        "    return y_pred[0]\n",
        "\n",
        "\n",
        "pred_text('همه چی عالی بود')"
      ],
      "metadata": {
        "id": "14OwYkX_HhnU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e49ae65-d30b-4b10-b17b-a34158e7696e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_num = np.argmax(y_test, axis=1)\n",
        "lstm_preds = model.predict(x_test)\n",
        "lstm_preds = np.argmax(lstm_preds, axis=1)\n",
        "get_metrics(y_test_num, lstm_preds)"
      ],
      "metadata": {
        "id": "RWlfIxjoHset",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35e6f0ce-b88c-420a-8b02-07b235ec3517"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.5544503953073195, 'balance_accuracy': 0.42878316690278295}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **using transformers and bert**"
      ],
      "metadata": {
        "id": "M9NMJt9gLdOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, AutoModel\n",
        "config = AutoConfig.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
        "model = AutoModel.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\", output_hidden_states=True)\n",
        "model = model.to(device)\n",
        "\n",
        "def bert_emb(text):   \n",
        "    encoded_input = tokenizer(text, return_tensors='pt').to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(**encoded_input)\n",
        "    return output['pooler_output'].detach().cpu().numpy()[0]\n",
        "\n",
        "bert_emb('رییس جمهور ایران به قطر رفت')"
      ],
      "metadata": {
        "id": "QZnOqrGdLjlO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}